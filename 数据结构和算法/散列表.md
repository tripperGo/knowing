## 1 散列思想

​	散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，有数组演化而来。没有数组、就没有散列表。

## 2 散列冲突

​	我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。

​	构造一个散列函数，应符合以下三点基本要求：

- 散列函数计算得到的散列值是一个非负整数；
- 如果 key1 = key2，那 hash(key1) == hash(key2)；
- 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。

​	因为数组下标是从0开始非负整数，所以hash(key)函数得到的散列值应该是一个非负整数；第二点也没问题；第三点设计到hash冲突问题，世界上几乎没有hash方法可以完全避免hash冲突，我们要解决hash冲突需要通过其他途径解决。主要有开放寻址法和链表法：

### 2.1 开放寻址法

​	当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。
​    <img src="https://static001.geekbang.org/resource/image/5c/d5/5c31a3127cbc00f0c63409bbe1fbd0d5.jpg" alt="avatar" style="zoom:50%;" />

​	从图中可以看出，散列表的大小为 10，在元素 x 插入散列表之前，已经 6 个元素插入到散列表中。x 经过 Hash 算法之后，被散列到位置下标为 7 的位置，但是这个位置已经有数据了，所以就产生了冲突。于是我们就顺序地往后一个一个找，看有没有空闲的位置，遍历到尾部都没有找到空闲的位置，于是我们再从表头开始找，直到找到空闲位置 2，于是将其插入到这个位置。

​	在散列表中查找元素的过程有点儿类似插入过程。我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。

<img src="https://static001.geekbang.org/resource/image/91/ff/9126b0d33476777e7371b96e676e90ff.jpg" alt="avatar" style="zoom: 33%;" />

​	线性探测法其实存在很大的问题。当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。

- 二次探测

  所谓二次探测，跟线性探测很像，线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2……而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+12，hash(key)+22……

- 双重探测

  所谓双重散列，意思就是不仅要使用一个散列函数。我们使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。

​	不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。我们用装载因子（load factor）来表示空位的多少。

> ***装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。***

#### 2.1.1 LinkedHashMap

> **LinkedHashMap使用开放寻址法来解决hash冲突。**

​	开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度。而且，这种方法实现的散列表，序列化起来比较简单。链表法包含指针，序列化起来就没那么容易。你可不要小看序列化，很多场合都会用到的。我们后面就有一节会讲什么是数据结构序列化、如何序列化，以及为什么要序列化。

​	用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。

​	**当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java 中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。**

### 2.2 链表法

​	链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。我们来看这个图，在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。

![avatar](https://static001.geekbang.org/resource/image/a4/7f/a4b77d593e4cb76acb2b0689294ec17f.jpg)

​	当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。当查找、删除一个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。那查找或删除操作的时间复杂度是多少呢？

​	实际上，这两个操作的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。

#### 2.2.1 链表法优化

​	链表法比起开放寻址法，对大装载因子的容忍度会更高。开放寻址法只适用装载因子小于1的情况，当装载因子接近1的时候，开放寻址法会有大量的散列冲突，导致大量的探测、再散列等，性能会降低很多。但是对于链表法来说，只要散列函数的值随机分布，即使装载因子的值变为了10，那么也只是链表的长度加长了而已，虽然查找效率有有所下降，但是比起顺序查找还是快了很多。

​	链表法如果将链表改造为一个更加高效的数据结构，比如：跳表、红黑树。那么即使出现散列冲突，在极端情况下所有的数据都散列到一个桶内，那么最差情况下的查找时间也只不过是O(log n)。这样也就有效的避免了散列碰撞攻击。

### 2.3 扩容的效能提升

​	如果散列表的大小为1GB，这时候想要把散列表扩大为2GB的容量，那么就需要对1GB的数据重新计算hash值，这是一个非常耗时且另用户无法接受的事情。

​	解决方案就是把扩容操作分批完成：

- 当装载因子达到阈值之后，我们只申请新空间，但是不把老数据重新hash，不迁移数据；
- 当有新数据插入时，将新数据进行hash并插入新空间，并从老空间中取出一个数据进行hash并放入新空间；
- 之后每次插入操作、都重复第二步，知道老的散列表中没有了数据，则将老散列表空间释放。

​	对于这种设计的查询操作，需要先查询新的散列表、再查询老的散列表。

## 3 HashMap的设计

### 3.1 初始大小

​	HashMap默认的初始大小为16，可以根据业务需求赋值一个初始值，尽量避免扩容操作；

### 3.2 装载因子和动态扩容

​	装载因子默认是0.75，当HashMap中元素个数超过散列表容量*0.75之后，就会启动扩容操作，每次都会扩展为原来的2倍大小。

### 3.3 散列冲突解决方法

​	HashMap采用链表法解决Hash冲突。在JDK1.8版本中，引入红黑树，当链表长度大于8时，链表转换为红黑树。这时候利用红黑树快速增删改擦汗的特性、提高HashMap的性能。当红黑树节点个数少于8时，又会将红黑树转换为链表。因为在数据量较小的情况下，红黑树需要维护平衡，比起链表来性能并没有明显的优势。

### 3.4 散列函数

```java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

```java
public int hashCode() {
    int h = hash;
    if (h == 0 && value.length > 0) {
        char val[] = value;

        for (int i = 0; i < value.length; i++) {
            h = 31 * h + val[i];
        }
        hash = h;
    }
    return h;
}
```

## 4 哈希算法

> 将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法

- 从哈希值不能反向推导出原始数据
- 对输入数据非常敏感，哪怕只是修改了一个bit，最后得到的哈希值也大不相同
- 散列冲突的概率要低，对于不同的原始数据，哈希值相同的概率非常小
- 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速的计算出哈希值

### 4.1 安全加密

​	常用的安全加密Hash算法有：MD5、SHA、DES、AES等

​	**鸽巢原理**：如果有10个鸽巢，11只鸽子，那么肯定至少有一个鸽巢中鸽子数量是大于1的。

​	同理，hash算法是无法避免hash冲突的。比如MD5算法，hash值是一个128位的二进制串，所能表示的数据最多有2^128个，而我们要hash的数据是无穷的。基于鸽巢原理，如果我们要对2^128+1个数据进行hash，那么必然会存在相同hash值的情况。一般情况下、hash值越长的hash算法，hash冲突的概率越低。

### 4.2 唯一标识

​	比如从海量图库中搜索一张图片是否存在，我们不能够依靠图片名或者图片的二进制码串作比较，我们可以为图片取一个唯一标识。比如我们可以取图片的二进制码串开头100个字节、中间100个字节、结尾100个字节，然后将这300个字节放在一起进行hash，得到的hash值作为唯一标识。通过这个唯一标识来判定图片是否在图库中。

### 4.3 数据校验

​	与图片的二进制码串进行hash一样，也是通过生成一个唯一标识的方式、来校验数据是否是一个。

### 4.4 散列函数

​	散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表的性能。不过，相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决。

### 4.5 负载均衡

​	负载均衡算法有轮询、随机、加权轮询等。如果要实现一个会话粘滞的负载均衡算法（同一个客户端在一次会话的所有请求都路由到同一个服务器上），可以借助hash算法来实现。

​	通过hash算法，对客户端的IP地址或者会话ID计算hash值，将取得的hash值与服务器列表的大小进行取模运算，最终得到的值就是被路由到的服务器编号。这样、我们就可以把同一个IP过来的所有请求都路由到同一个后端服务器上。

### 4.6 数据分片

- **如何统计搜索关键词出现的次数**

  > 假如我们有 1T 的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？

  ​	这个问题有两个难点，一个是搜索日志很大，没办法将所有日志放到同一台机器的内存中；第二个是如果用一台机器处理，那么速度会很慢。

  ​	**我们可以对数据进行分片，然后采用多台机器处理的方法，来提高速度。**为了提高速度，我们用n台机器并行处理。首先从日志文件服务器一次读取出每个搜索关键词，并且通过hash算法计算hash值，然后再跟n取模，最终得到的值，就是应该被分配到的机器编号。

  ​	这样hash值相同的搜索关键词就会被分配到同一台机器上。也就是说同一个搜索关键词会被分配到同一个机器上。每个机器分别计算关键词出现的次数，最终合并起来就是最终的结果。

  > 实际上MapReduce就是这个设计思想

- **如何快速判断图片是否在图库中**

  > 假设我们图库中有1亿张图片

  ​	1亿张图片在一台服务器上是无法存储的，只能考虑对数据进行分片存储。首先我们准备n台服务器，每次从图库中取出一张图片，进行hash计算获取唯一标识，然后与机器个数n取模，得到的值就是图片要分配的服务器编号，然后将这个图片的hash值和图片路径发往对应的机器构建散列表。

  ​	当我们要判断一张图片是否存在图库中的时候，同样可以通过hash计算，计算这个图片的hash值，然后与机器个数n进行取模。假设得到的值为k，那么就去编号为k的机器构建的散列表中查询。

### 4.7 分布式缓存

​	如果我们有海量的数据需要缓存起来，提升数据的读取、写入能力，这时候一台机器肯定是不够的，需要将数据分布到多台机器上。

​	首先如何决定数据应该存放到哪台机器上？此时可以借助hash算法，通过hash算法获取hash值，然后与机器个数取模，这个最终值就是应该缓存的缓存机器编号。

​	如果这时候缓存服务器不够用了，需要新加机器了，那么就需要考虑扩容，如果服务器由10台变为了13台，那么原来的数据是以10取模的，现在要以13取模，这就导致原本的缓存数据会失效。这时候所有的数据请求都会穿透缓存、直接访问数据库。这就可能发生雪崩效应，压垮数据库。这时候需要采用**一致性哈希算法 **。

​	假设我们有 k 个机器，数据的哈希值的范围是[0, MAX]。我们将整个范围划分成 m 个小区间（m 远大于 k），每个机器负责 m/k 个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。