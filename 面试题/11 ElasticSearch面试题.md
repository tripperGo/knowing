## 1 场景介绍

​	ES 集群架构 13 个节点，索引根据通道不同共 20+索引，根据日期，每日递增 20+，索引：10 分片，每日递增 300w+数据，每个通道每天索引大小控制：15GB 之内。

### 1.1 设计阶段调优

- 仅针对需要分词的字段，合理的设置分词器；
- Mapping 阶段充分结合各个字段的属性，是否需要检索、是否需要存储等；
- 每天凌晨定时对索引做 force_merge 操作，以释放空间；

### 1.2 写入调优

- 写入前副本数设置为 0；

- 写入前关闭 refresh_interval 设置为-1，禁用刷新机制；

- 写入过程中：采取 bulk 批量写入；

- 写入后恢复副本数和刷新间隔；

- 尽量使用自动生成的 id。

### 1.3 查询调优

- 禁用批量 terms（成百上千的场景）
- 充分利用倒排索引机制，能 keyword 类型尽量 keyword；
- 数据量大时候，可以先基于时间敲定索引再检索；

### 1.4 其他调优

​	部署调优，业务调优等等。

​	比如在ES只存放关键必要字段，Hbase存储完整的数据。根据条件查询出ID之后，再去Hbase中查询完成的数据等等。

### 1.5服务器优化

- 关闭缓存 swap;
- 堆内存设置为：Min（节点内存/2, 32GB）;
- 线程池+队列大小根据业务需要做调整；
- 如果有条件选用镜像阵列条带磁盘，增加单节点性能以及避免单节点存储故障。

## 2 倒排索引

​	通过分词策略，形成分词和文章ID集合的映射关系表，可以再O(1)的时间复杂度内根据分词找到对应的文章，检索效率特别高。

​	倒排索引的底层实现是基于：**FST**（`Finite State Transducer`）数据结构。

​	lucene 从 4+版本后开始大量使用的数据结构是` FST`。`FST` 有两个优点：

- 空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间；

- 查询速度快。O(len(str))的查询时间复杂度。

|  id  | name | sex  |
| :--: | :--: | :--: |
|  1   |  A   |  男  |
|  2   |  B   |  女  |
|  3   |  C   |  男  |

这张表的倒排索引就是：

name:

| A    | 1    |
| ---- | ---- |
| B    | 2    |
| C    | 3    |

sex:

|  男  | [1,3] |
| :--: | :---: |
|  女  |  [2]  |

​	**倒排索引是per field的，一个字段有一个自己的倒排索引**。男、女 这些叫做 term，而[1,3]就是posting list。Posting list就是一个int的数组，存储了所有符合某个term的文档id。

​	如果要搜索的是一堆无序的词：Carla,Sara,Elin,Ada,Patty,Kate,Selena。那么如果要找出某一个term（项）一定很慢，因为term是无序的，需要全部遍历一遍才能找到term。

​	如果词是有序的，那么我们可以根据二分查找，比全遍历更快地找出目标的term。这个就是term directonary，在log(n)的时间复杂度内完成检索，也就是进行log(n)次磁盘读取。但是磁盘随机读操作是非常耗时的，一般一次随机访问大概需要10ms。所以应该尽量少得读磁盘，有必要把一些数据缓存到内存中。但是整个term dictionary本身又太大了，无法完整地放到内存里。于是就有了term index。term index有点像一本字典的大的章节表。

​	如果所有的term都是英文字符的话，可能这个term index就真的是26个英文字符表构成的了。但是实际的情况是，term未必都是英文字符，term可以是任意的byte数组。而且26个英文字符也未必是每一个字符都有均等的term，比如x字符开头的term可能一个都没有，而s开头的term又特别多。实际的term index是一棵trie 树：

​	这棵树不会包含所有的term，它包含的是term的一些前缀。通过term index可以快速地定位到term dictionary的某个offset，然后从这个位置再往后顺序查找。再加上一些压缩技术（搜索 Lucene Finite State Transducers） term index 的尺寸可以只有所有term的尺寸的几十分之一，使得用内存缓存整个term index变成可能。整体上来说就是这样的效果。

​	Elasticsearch比mysql快的原因是：Mysql只有term dictionary这一层，是以b-tree排序的方式存储在磁盘上的。检索一个term需要若干次的random access的磁盘操作。而Lucene在term dictionary的基础上添加了term index来加速检索，term index以树的形式缓存在内存中。从term index查到对应的term dictionary的block位置之后，再去磁盘上找term，大大减少了磁盘的random access次数。

​	额外值得一提的两点是：term index在内存中是以FST（finite state transducers）的形式保存的，其特点是非常节省内存。Term dictionary在磁盘上是以分block的方式保存的，一个block内部利用公共前缀压缩，比如都是Ab开头的单词就可以把Ab省去。这样term dictionary可以比b-tree更节约磁盘空间。

## 3 ES索引数据多了、如果调优

​	索引数据的规划，应在前期做好规划，正所谓“设计先行，编码在后”，这样才能有效的避免突发的事件、比如数据激增导致集群处理能力不足引发的线上客户检索或者其他业务受到影响。

- 动态索引层面

  如果是日志索引，应该基于模板+时间+rollover api滚动创建索引，比如：log_index_时间戳，每天递增数据。这样做的好处是：不至于数据量激增导致单个索引数据量非常大，一旦单个索引很大，那么存储等各种风险可能随之而来。

- 存储层面

  冷热数据分离存储，热数据（比如最近 3 天或者一周的数据），其余为冷数据。

  对于冷数据不会再写入新数据，可以考虑定期 force_merge 加 shrink 压缩操作，节省存储空间和检索效率。

- 部署层面

  结合 ES 自身的支持动态扩展的特点，动态新增机器的方式可以缓解集群压力

## 4 ES的master选主

- Elasticsearch 的选主是 ZenDiscovery 模块负责的，主要包含 Ping（节点之间通过这个 RPC 来发现彼此）和 Unicast（单播模块包含一个主机列表以控制哪些节点需要 ping 通）这两部分；
- 对所有可以成为 master 的节点（node.master: true）根据 节点ID 字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第 0 位）节点，暂且认为它是 master 节点。
- 如果对某个节点的投票数达到一定的值（可以成为 master 节点数 n/2+1）并且该节点自己也选举自己，那这个节点就是 master。否则重新选举一直到满足上述条件。
- master 节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data 节点可以关闭 http 功能。

<span style='color:red'>**或者:**</span>

ES的选主有两个前置条件：

- 只有候选主节点（master:true）的节点才能成为主节点
- 最小主节点数（min_master_nodes）的目的是防止脑裂

选主流程大致分为两步：

- 确认候选主节点数是否达标，elasticsearch.yml中设置的discovery.zen.minimum_master_nodes；
- 比较，判断节点是否具有master资格，具备候选资格的节点优先返回
  - 如果两个节点都具有资格竞选master，那么默认取id值小的为主节点。

### 4.1 选主

如果节点共20个，其中10个选择了一个master，另外10个选择了另一个master，应该怎么办？

- 当集群master候选的数量 >= 3时，可以通过设置最少投票通过数量 `discovery.zen.minimum_master_nodes`来解决，超过所有候选节点一半以上来解决脑裂问题
- 当候选数量为2时，只能修改为唯一的一个master候选，其他作为Data节点，避免脑裂问题。

## 5 ES索引文档的过程

<img src="..\cache\img\16f3cd47d2b0df73.jpg" style="zoom:80%;" />

- 客户写的某个线程发送请求，写入数据

- 节点 1 接受到请求后，使用文档_id 来确定文档属于分片 0。请求会被转到另外的节点，假定节点 3。因此分片 0 的主分片分配到节点 3 上。

  - 借助hash路由算法，路由算法就是根据路由和文档ID计算目标分片ID的过程

    ```java
    1shard = hash(_routing) % (num_of_primary_shards)
    ```

- 节点 3 在主分片上执行写操作，如果成功，则将请求并行转发到节点 1和节点 2 的副本分片上，等待结果返回。所有的副本分片都报告成功，节点 3 将向协调节点（节点 1）报告成功，节点 1 向请求客户端报告写入成功。

## 6 ES检索的过程

搜索过程分为query then fetch两个阶段。

query阶段主要是定位到位置，但是不取数据；

fetch阶段根据路由节点的位置、获取所有数据，并返回给客户端。



query节点步骤大致分为3步：

- 假设一个索引共有5个分片，一次请求会命中（主分片或者副本）中的一个
- 每个分片会在本地进行查询，结果返回到本地有序的优先级队列中
- 本地优先级队列会发送到协调节点（也就是master），协调节点会产生一个全局的有序列表。



---

**`TF/IDF`**

* **TF**

  **词频(Term Frequency)**。词频是针对一篇文档进行描述的。一个词在某篇文档中出现的频率越高，则TF值越高。
  $$
  TF_w={N_w \over N}
  $$
  翻译成中文：
  $$
  TF_w = {某个词在某篇文档中出现的次数 \over 该文档中所有词的总数}
  $$

* **IDF**

  **逆文本频率指数(Inverse Document Frequency)**，是针对所有词库中多个文档进行描述的。某个词在词库中所有文档中出现的次数越少，越能够代表该文档，IDF值越高。
  $$
  IDF_w = log({Y \over Y_w + 1})
  $$
  翻译成中文：
  $$
  IDF_w = log_{10}({语料库中总文档数 \over 包含该词条的文档数 + 1})，分母之所以要加1，是为了避免分母为0
  $$

- 汇总过程。某一特定文档的高频词，结合该词在所有词库文档中的低文档频率，可以产出高权重的TF/IDF评分结果，因此TF/IDF倾向于过滤掉常见的词，保留重要的词。
  $$
  TF_w - IDF_w = TF_w * IDF_w
  $$
  

**Lucene评分机制**
$$
score(q, d) = coord(q, d)*queryNorm(q)*\sum_{t \ in \ q}(tf(t \ in \ d)*idf(t)^2*t.getBoost()*norm(t,d))
$$

- coord(q,d) 评分因子，基于文档中出现查询项的个数。越多的查询项在一个文档中，说明文档的匹配程度越高。
- queryNorm(q)查询的标准查询
- tf(t in d) 指项t在文档d中出现的次数frequency。具体值为次数的开根号。
- idf(t) 反转文档频率, 出现项t的文档数docFreq
- t.getBoost 查询时候查询项加权
- norm(t,d) 长度相关的加权因子